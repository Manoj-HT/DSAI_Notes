{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffece096",
   "metadata": {},
   "source": [
    "# C1: INTRODUCTION TO NEURAL NETWORKS\n",
    "\n",
    "## Neuron\n",
    "- A neuron is a mathematical function that takes inputs, processes them, and produces an output.\n",
    "- It is a computational unit that:\n",
    "    1. Receives inputs\n",
    "    2. Applies weights to each input\n",
    "    3. Sums them up and adds a bias term  \n",
    "        - $\\mathrm{z} = \\sum(w_i \\cdot x_i) + b$\n",
    "    4. Passes the result to an activation function to introduce non-linearity  \n",
    "        - $\\mathrm{y} = f(z)$\n",
    "- Neurons are also called units or nodes.\n",
    "\n",
    "## Neural Network\n",
    "- A neural network is a computational model inspired by how the human brain works.\n",
    "- It consists of layers of nodes/neurons connected by weighted links.\n",
    "- Layers:\n",
    "    - **Input layer** → takes raw data\n",
    "    - **Hidden layers** → extract features through transformations\n",
    "    - **Output layer** → gives the prediction\n",
    "- During training, it adjusts weights using **backpropagation** with an optimizer like gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4a25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron raw output:  2.90\n"
     ]
    }
   ],
   "source": [
    "# Inputs: restaurant features\n",
    "price = 3 # scale 1 to 5 -> cheap to expensive\n",
    "rating = 4 # scale 1 to 5 -> bad to excellent\n",
    "\n",
    "# Weights: How much you care about each\n",
    "w_price = -0.8 # you prefer cheaper, so negative weights \n",
    "w_rating = 1.2 # you prefer good ratings, so positive weights\n",
    "\n",
    "# Bias: general tendency to eat out\n",
    "bias = 0.5\n",
    "\n",
    "# Neuron calculation\n",
    "output = (price *  w_price) + (rating * w_rating) + bias\n",
    "print(f\"Neuron raw output: {output: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85cf16",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "- **Definition**: An activation function decides whether a neuron should be activated, i.e., whether it should pass the information forward.  \n",
    "- Activation functions introduce **non-linearity**, allowing the network to learn complex patterns instead of just passing information in a straight line.  \n",
    "- Without activation functions, a neural network would behave like a simple linear model, regardless of its depth.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "#### Sigmoid\n",
    "- **Formula**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$  \n",
    "- **Range**: (0, 1)  \n",
    "- **Pros**:  \n",
    "    - Useful for probability outputs  \n",
    "    - Provides a smooth gradient  \n",
    "- **Cons**:  \n",
    "    - Suffers from the **vanishing gradient problem** (for very large or very small $x$)  \n",
    "    - Can lead to **slow convergence**\n",
    "\n",
    "#### Tanh (Hyperbolic Tangent)\n",
    "- **Formula**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$  \n",
    "- **Range**: (-1, 1)  \n",
    "- **Pros**:  \n",
    "    - Output is **zero-centered** (better than sigmoid for hidden layers)  \n",
    "- **Cons**:  \n",
    "    - Still suffers from the **vanishing gradient problem**\n",
    "\n",
    "#### ReLU (Rectified Linear Unit)\n",
    "- **Formula**: $f(x) = \\max(0, x)$  \n",
    "- **Range**: (0, $\\infty$)  \n",
    "- **Pros**:  \n",
    "    - Fast to compute  \n",
    "    - Helps reduce the vanishing gradient problem  \n",
    "    - Widely used in deep networks  \n",
    "- **Cons**:  \n",
    "    - Can cause **\"dead neurons\"** (if a neuron's output is always $\\leq 0$, it stops learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd560cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoidal functions for 3.3 ans 3.5\n",
      "0.9644288107273639  Close to 1\n",
      "0.039165722796764356 Close to 0\n",
      "\n",
      "\n",
      "Tanh function for 2.0 and -2.0\n",
      "0.9640275800758169  Positive outcome\n",
      "-0.9640275800758169  Negative outcome\n",
      "\n",
      "\n",
      "3.3  Keeps positive\n",
      "0  Zeroes out\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoidal activation function\n",
    "def sigmoidal(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "print(\"Sigmoidal functions for 3.3 ans 3.5\")\n",
    "print(sigmoidal(3.3), \" Close to 1\")\n",
    "print(sigmoidal(-3.2), \"Close to 0\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "print(\"Tanh function for 2.0 and -2.0\")\n",
    "print(tanh(2.0), \" Positive outcome\")\n",
    "print(tanh(-2.0), \" Negative outcome\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "print(relu(3.3), \" Keeps positive\")\n",
    "print(relu(-2.0), \" Zeroes out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227233ef",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "\n",
    "- A **feed-forward neural network (FNN)** is a type of artificial neural network.  \n",
    "- In this network, information moves only in one direction — from input to output.  \n",
    "- Unlike recurrent networks, it does not have cycles or feedback connections.  \n",
    "\n",
    "### Layers\n",
    "- **Input Layer**: Takes raw data  \n",
    "- **Hidden Layers**: Perform computations using activation functions  \n",
    "- **Output Layer**: Produces the final prediction or classification  \n",
    "\n",
    "### Training\n",
    "- Uses **backpropagation** and **gradient descent** to update weights  \n",
    "\n",
    "### Usage\n",
    "- Common applications include:  \n",
    "    - Classification  \n",
    "    - Regression  \n",
    "    - Feature extraction (e.g., image recognition, speech processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c547b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output:  [4.3 0. ]\n",
      "Final decision (probability) 0.86056612703835\n"
     ]
    }
   ],
   "source": [
    "# Input features (price, rating)\n",
    "inputs = np.array([3, 4])\n",
    "\n",
    "#Layer 1: 2 neurons\n",
    "weights1 = np.array([[0.2, 0.8],    # Neuron 1 weights\n",
    "                     [0.6, -0.5]])  # Neuron 2 weights\n",
    "bias1 = np.array([0.5, -0.2])\n",
    "\n",
    "layer1_output = np.dot(inputs, weights1.T) + bias1\n",
    "layer1_output = np.maximum(layer1_output, 0) # ReLU activation\n",
    "\n",
    "print(\"Layer 1 output: \", layer1_output)\n",
    "\n",
    "# Layer 2: 1 neuron -> Final decision\n",
    "weights2 = np.array([0.4, 0.9])\n",
    "bias2 = 0.1\n",
    "\n",
    "output = np.dot(layer1_output, weights2) + bias2\n",
    "output = 1 / (1 + np.exp(-output))\n",
    "print(\"Final decision (probability)\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716669f7",
   "metadata": {},
   "source": [
    "## Training a Neural Network\n",
    "\n",
    "A neural network can be trained by adjusting its internal weights based on data.  \n",
    "\n",
    "### Steps of Training\n",
    "1. Input data is fed into the network.  \n",
    "2. The network computes an output.  \n",
    "3. The output is compared with the actual label.  \n",
    "4. A **loss function** calculates how wrong the network is.  \n",
    "5. The gradient of the loss with respect to the weights is computed (slope of the error surface).  \n",
    "6. This gradient indicates the direction to reduce error.  \n",
    "7. An **optimizer** (e.g., SGD, Adam) updates the weights:  \n",
    "\n",
    "   - $w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}$  \n",
    "\n",
    "   where:  \n",
    "   - $w$ = weight  \n",
    "   - $\\eta$ = learning rate (controls step size)  \n",
    "   - $L$ = loss function  \n",
    "\n",
    "8. Repeat the process multiple times until the network’s predictions improve.  \n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "- **Definition**: A loss function measures how well a model is performing.  \n",
    "- It compares the model’s predictions with the actual target values.  \n",
    "- **Goal**: Minimize the loss during training so the model learns better.  \n",
    "\n",
    "### Types of Loss Functions\n",
    "\n",
    "#### 1. Regression Losses\n",
    "- **Mean Squared Error (MSE):**  \n",
    "  - Formula: $L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$  \n",
    "  - Penalizes larger errors more heavily.  \n",
    "- **Mean Absolute Error (MAE):**  \n",
    "  - Formula: $L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$  \n",
    "  - More robust to outliers, less harsh than MSE.  \n",
    "- **Huber Loss:**  \n",
    "  - Formula:  \n",
    "\n",
    "    $$\n",
    "    L_\\delta(y, \\hat{y}) =\n",
    "    \\begin{cases} \n",
    "      \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "      \\delta \\cdot |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$  \n",
    "\n",
    "  - Combines the advantages of MSE and MAE.  \n",
    "  - Less sensitive to outliers compared to MSE.  \n",
    "\n",
    "#### 2. Classification Losses\n",
    "- **Binary Cross-Entropy (Log Loss):**  \n",
    "  - Formula: $L(y, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^n \\big[ y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\big]$  \n",
    "  - For binary classification problems (e.g., spam vs. not spam).  \n",
    "- **Categorical Cross-Entropy:**  \n",
    "  - Formula: $L(y, \\hat{y}) = -\\sum_{i=1}^C y_i \\log(\\hat{y}_i)$  \n",
    "  - For multi-class classification (e.g., cat/dog/horse).  \n",
    "- **Sparse Categorical Cross-Entropy:**  \n",
    "  - Same as categorical cross-entropy, but labels are integers instead of one-hot vectors.  \n",
    "\n",
    "#### 3. Advanced Losses\n",
    "- **Hinge Loss:** Commonly used for SVMs.  \n",
    "- **KL Divergence (Kullback–Leibler):**  \n",
    "  - Formula: $D_{KL}(P \\| Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$  \n",
    "  - Measures differences between probability distributions (useful in VAEs, knowledge distillation).  \n",
    "- **Contrastive Loss / Triplet Loss:** Used in face recognition and metric learning.  \n",
    "- **Dice Loss / IoU Loss:** Used in image segmentation tasks (e.g., medical imaging).  \n",
    "- **CTC Loss (Connectionist Temporal Classification):** Used for sequence problems like speech recognition.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f96f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.15000002086162567\n",
      "MAE Loss: 0.3500000238418579\n",
      "Huber Loss: 0.07500001043081284\n",
      "Binary Cross-Entropy Loss: 0.18388254940509796\n",
      "Categorical Cross-Entropy Loss: 0.5162140727043152\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# True values (targets)\n",
    "y_true_reg = torch.tensor([2.5, 0.0, 2.1, 7.8])\n",
    "y_pred_reg = torch.tensor([3.0, -0.5, 2.0, 7.5])\n",
    "\n",
    "y_true_cls = torch.tensor([1, 0, 1])       # classification labels\n",
    "y_pred_cls = torch.tensor([[0.3, 0.7],     # predicted probabilities\n",
    "                           [0.6, 0.4],\n",
    "                           [0.2, 0.8]])\n",
    "\n",
    "# ----- Regression Losses -----\n",
    "mse = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "huber = nn.HuberLoss()\n",
    "\n",
    "print(\"MSE Loss:\", mse(y_pred_reg, y_true_reg).item())\n",
    "print(\"MAE Loss:\", mae(y_pred_reg, y_true_reg).item())\n",
    "print(\"Huber Loss:\", huber(y_pred_reg, y_true_reg).item())\n",
    "\n",
    "# ----- Classification Loss -----\n",
    "# Binary cross entropy\n",
    "bce = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "y_true_bin = torch.tensor([1., 0., 1.])  # binary labels\n",
    "y_pred_bin = torch.tensor([0.9, 0.2, 0.8])  # predicted probs\n",
    "print(\"Binary Cross-Entropy Loss:\", bce(y_pred_bin, y_true_bin).item())\n",
    "\n",
    "# Categorical cross entropy (for multi-class classification)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "print(\"Categorical Cross-Entropy Loss:\", cross_entropy(y_pred_cls, y_true_cls).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d0c2e",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "- **Definition**: Optimisation is the process of updating model parameters (weights and biases) to minimize the loss function.  \n",
    "- **Goal**: Find the best set of weights so that predictions are accurate.  \n",
    "- This happens during the training loop:  \n",
    "  **Forward pass → Compute loss → Backward pass (gradients) → Update weights**  \n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent is the most common optimization algorithm.  \n",
    "\n",
    "### Core Idea\n",
    "- Adjust weights in the **opposite direction of the gradient** (slope of the loss function).  \n",
    "\n",
    "### Update Rule\n",
    "- $w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}$  \n",
    "\n",
    "Where:  \n",
    "- $w$ = parameter (weight)  \n",
    "- $L$ = loss function  \n",
    "- $\\eta$ = learning rate (step size)  \n",
    "\n",
    "### Types of Gradient Descent\n",
    "1. **Batch Gradient Descent**  \n",
    "   - Uses the entire dataset per update  \n",
    "   - Very accurate but very slow  \n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**  \n",
    "   - Updates weights for every single sample  \n",
    "   - Faster but noisy  \n",
    "\n",
    "3. **Mini-Batch Gradient Descent**  \n",
    "   - Uses small batches (e.g., 32, 64 samples)  \n",
    "   - Best trade-off → most widely used in deep learning  \n",
    "\n",
    "### Limitations\n",
    "- **Local minima / saddle points** → model can get stuck  \n",
    "- **Slow convergence** → if learning rate is too small  \n",
    "- **Overshooting** → if learning rate is too large  \n",
    "\n",
    "### Beyond Basic Gradient Descent\n",
    "- **SGD with Momentum** → adds inertia to updates, helps escape local minima  \n",
    "- **RMSProp** → adjusts learning rate per parameter  \n",
    "- **Adam** → combines momentum + adaptive learning rate  \n",
    "- **Adagrad / Adadelta** → adaptive optimizers useful for sparse data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "875fff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.0819\n",
      "Epoch [40/100], Loss: 0.0298\n",
      "Epoch [60/100], Loss: 0.0264\n",
      "Epoch [80/100], Loss: 0.0234\n",
      "Epoch [100/100], Loss: 0.0208\n",
      "Learned weight: 1.8803331851959229\n",
      "Learned bias: 0.3518347144126892\n"
     ]
    }
   ],
   "source": [
    "# Data (x and y = 2x)\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Simple linear model: y = wx\n",
    "model = nn.Linear(1, 1)   # 1 input, 1 output\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()   # clear old gradients\n",
    "    loss.backward()         # compute gradients\n",
    "    optimizer.step()        # update weights\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final learned weight and bias\n",
    "[w, b] = model.parameters()\n",
    "print(\"Learned weight:\", w.item())\n",
    "print(\"Learned bias:\", b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80767f",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- **Definition**: Backpropagation is an algorithm used to train neural networks.  \n",
    "- It calculates how much each weight and bias contributed to the overall error (loss) by applying the **chain rule of calculus**, and then updates them to reduce the error.  \n",
    "\n",
    "### Need for Backpropagation\n",
    "- **Efficiency**: Reuses intermediate results from the forward pass when computing gradients.  \n",
    "- **Scalability**: Works for deep networks where manual gradient calculation would be impossible.  \n",
    "\n",
    "### Steps of Backpropagation\n",
    "1. **Forward Pass**  \n",
    "   - Inputs pass through the network layer by layer to produce an output $\\hat{y}$.  \n",
    "   - The predicted output $\\hat{y}$ is compared with the true output $y$ using a loss function.  \n",
    "\n",
    "2. **Backward Pass**  \n",
    "   - Compute the gradient of the loss with respect to the output.  \n",
    "   - Propagate this error backward through the network using the **chain rule**, layer by layer.  \n",
    "   - Compute gradients for each weight and bias.  \n",
    "\n",
    "3. **Update Parameters**  \n",
    "   - Use **gradient descent** (or its variants) to update weights and biases based on the computed gradients.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39c011",
   "metadata": {},
   "source": [
    "Complete Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf84a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss =  0.1821\n",
      "Epoch 20: Loss =  0.0534\n",
      "Epoch 30: Loss =  0.0157\n",
      "Epoch 40: Loss =  0.0046\n",
      "Epoch 50: Loss =  0.0013\n",
      "Epoch 60: Loss =  0.0004\n",
      "Epoch 70: Loss =  0.0001\n",
      "Epoch 80: Loss =  0.0000\n",
      "Epoch 90: Loss =  0.0000\n",
      "Epoch 100: Loss =  0.0000\n",
      "Prediction for input 5.0:  9.996416091918945\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 1. Inputs with Dummy  dataset (X -> y)\n",
    "X = torch.tensor([[0.0], [1.0], [2.0], [3.0]], dtype=torch.float32) # input\n",
    "y = torch.tensor([[0.0], [2.0], [4.0], [6.0]], dtype=torch.float32) # expected output y = 2x\n",
    "\n",
    "# 2. Define a simple neural netowrk\n",
    "model = nn.Sequential(nn.Linear(1,1)) # 1 input -> 1 output\n",
    "\n",
    "# 3. Define Loss function optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) # Stochastic Gradient Decent\n",
    "\n",
    "# 4. Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute prediction\n",
    "    y_pred = model(X)\n",
    "    # Compute loss (prediction vs actual)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backward pass: compute gradients\n",
    "    optimizer.zero_grad() # Reset old gradients\n",
    "    loss.backward() # compute new gradients\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    # Print progress\n",
    "    if(epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss.item(): .4f}\")\n",
    "\n",
    "# Test the trained model\n",
    "test_val = torch.tensor([5.0])\n",
    "print(\"Prediction for input 5.0: \", model(test_val).item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
