{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7e88bd",
   "metadata": {},
   "source": [
    "# C4: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae801e0",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- **Definition:** Creating new features from raw data, especially from unstructured sources like text, images, or audio.\n",
    "- **Examples:**\n",
    "  - **Text:** Word counts, TF-IDF scores, word embeddings (Word2Vec, GloVe, BERT)\n",
    "  - **Images:** Edges, color histograms, CNN features\n",
    "  - **Time Series:** Rolling averages, seasonality/trend indicators, Fourier transforms\n",
    "- **Need:** Converts raw/unstructured data into numeric features that can be effectively used by ML models.\n",
    "\n",
    "## Feature Transformation\n",
    "- **Definition:** Changing existing features into more useful forms to improve model performance.\n",
    "- **Common Techniques:**\n",
    "  1. Log transformation – Reduces skewness in data\n",
    "  2. Square root / Box-Cox transformations – Stabilize variance\n",
    "  3. Binning – Convert continuous values into categorical intervals\n",
    "  4. Polynomial features – Add interaction terms or higher-order terms\n",
    "  5. Encoding – One-hot encoding, label encoding for categorical variables\n",
    "  6. Discretization – Splitting continuous features into discrete buckets\n",
    "- **Need:** Helps improve linearity, model interpretability, and handling of non-linear relationships.\n",
    "\n",
    "## Feature Scaling\n",
    "- **Why Needed:** Some algorithms (e.g., Linear/Logistic Regression, SVM, K-Means, PCA, Neural Networks) are sensitive to feature scale.\n",
    "- **Techniques:**\n",
    "  - **Min-Max Normalization:**\n",
    "    - Formula: $\\mathrm{x'} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$\n",
    "    - Range: [0, 1]\n",
    "    - **Use Case:** When bounded values are required, e.g., image pixel scaling\n",
    "  - **Standardization (Z-score scaling):**\n",
    "    - Formula: $\\mathrm{z} = \\frac{x - \\mu}{\\sigma}$\n",
    "    - Mean = 0, Standard Deviation = 1\n",
    "    - **Use Case:** Preferred when data contains outliers\n",
    "  - **Robust Scaling (Median & IQR):**\n",
    "    - Formula: $\\mathrm{x'} = \\frac{x - \\text{median}(x)}{\\text{IQR}}$\n",
    "    - **Use Case:** Works well with heavy-tailed distributions and extreme outliers.\n",
    "\n",
    "## Feature Selection\n",
    "- **Goal:** Keep only the most informative features while removing irrelevant or noisy variables to reduce overfitting and improve efficiency.\n",
    "- **Methods:**\n",
    "  1. **Filter Methods:** Statistical tests (Chi-square, ANOVA, Mutual Information, Correlation thresholding)\n",
    "  2. **Wrapper Methods:**\n",
    "     - Forward Selection – Start with no features, add one at a time if model improves\n",
    "     - Backward Elimination – Start with all features, remove least significant step by step\n",
    "     - Stepwise Selection – Combination of forward and backward\n",
    "     - Recursive Feature Elimination (RFE) – Train model, remove least important features iteratively\n",
    "  3. **Embedded Methods:**\n",
    "     - L1 Regularization (Lasso)\n",
    "     - Tree-based models (e.g., Random Forest, XGBoost feature importance)\n",
    "- **Benefits:** Improves model accuracy, reduces computation, prevents overfitting, and enhances interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8504ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['bmi', 'bp', 's1', 's2', 's5']\n",
      "Ranking of Features: [6 2 1 1 1 1 4 3 1 5]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Dataset\n",
    "# ---------------------------\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Fit Linear Regression Model\n",
    "# ---------------------------\n",
    "model = LinearRegression()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Apply RFE\n",
    "# ---------------------------\n",
    "rfe = RFE(model, n_features_to_select=5)  # keep top 5 features\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Results\n",
    "# ---------------------------\n",
    "print(\"Selected Features:\", X.columns[rfe.support_].tolist())\n",
    "print(\"Ranking of Features:\", rfe.ranking_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
