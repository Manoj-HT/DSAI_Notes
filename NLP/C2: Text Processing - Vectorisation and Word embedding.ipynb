{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a564b325",
   "metadata": {},
   "source": [
    "# C2: TEXT PROCESSING - VECTORISATION AND WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dba9d5",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "- **Definition:** The process of converting text (words, sentences, documents) into numerical representations (vectors) so that ML models can process them.  \n",
    "- **Need:** Machines cannot understand raw text; they work with numbers.\n",
    "\n",
    "### Terminologies\n",
    "\n",
    "- **Corpus:** A collection of documents.  \n",
    "- **Vocabulary:** A unique set of words in the corpus.  \n",
    "- **Document-Term Matrix (DTM):** A representation where rows are documents and columns are words.  \n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- **Definition:** The process of breaking text into smaller units (tokens) before giving it to a model.  \n",
    "- **Token:** A token can be a word, subword, or even a character depending on the tokenizer.  \n",
    "- **Model Mapping:** The model converts each token into a number (ID) which maps to an embedding vector.  \n",
    "\n",
    "#### Need for Tokenization\n",
    "\n",
    "- Provides a consistent way to split text.  \n",
    "- Builds a vocabulary list mapping tokens to numbers.  \n",
    "\n",
    "#### Types of Tokenization\n",
    "\n",
    "1. **Word-Level Tokenization**  \n",
    "   - Splits text by spaces/punctuation.  \n",
    "   - Example: `\"I love NLP\"` $\\to$ `[\"I\", \"love\", \"NLP\"]`  \n",
    "   - **Problem:** Huge vocabulary; cannot handle new/rare words.  \n",
    "\n",
    "2. **Character-Level Tokenization**  \n",
    "   - Each character is treated as a token.  \n",
    "   - Example: `\"Chat\"` $\\to$ `[\"C\", \"h\", \"a\", \"t\"]`  \n",
    "   - **Advantage:** Very flexible.  \n",
    "   - **Disadvantage:** Sequences become very long.  \n",
    "\n",
    "3. **Subword-Level Tokenization**  \n",
    "   - Breaks rare words into smaller units while keeping common words intact.  \n",
    "   - Example: `\"unhappiness\"` $\\to$ `[\"un\", \"##happi\", \"##ness\"]`  \n",
    "     - `##` indicates that this piece continues a word.  \n",
    "   - **Advantage:** Can handle new words by combining smaller pieces.  \n",
    "\n",
    "### Common Techniques of Vectorization\n",
    "\n",
    "Some widely used methods to represent text as vectors:\n",
    "\n",
    "1. **One-Hot Encoding**  \n",
    "   - Represents each word as a binary vector.  \n",
    "   - Simple but results in high-dimensional sparse vectors.  \n",
    "   - No semantic meaning captured.  \n",
    "\n",
    "2. **Bag of Words (BoW)**  \n",
    "   - Represents text by word counts or frequencies.  \n",
    "   - Ignores word order.  \n",
    "   - **Limitation:** Cannot capture context.  \n",
    "\n",
    "3. **TF-IDF (Term Frequency–Inverse Document Frequency)**  \n",
    "   - Adjusts word counts by reducing the weight of common words and increasing the weight of rare words.  \n",
    "   - Better than BoW but still ignores word order.  \n",
    "\n",
    "4. **Word Embeddings**  \n",
    "   - Dense vector representations learned from large corpora (e.g., Word2Vec, GloVe).  \n",
    "   - Capture semantic meaning (e.g., \"king\" – \"man\" + \"woman\" ≈ \"queen\").  \n",
    "\n",
    "5. **Contextual Embeddings**  \n",
    "   - Dynamic embeddings generated from context (e.g., BERT, GPT).  \n",
    "   - The same word can have different vectors depending on its usage.  \n",
    "   - State-of-the-art for modern NLP.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63823e89",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "\n",
    "- **Definition:** Each word in the vocabulary is represented as a binary vector of size equal to the vocabulary.  \n",
    "- Only one position is `1` (indicating the word’s index), and all other positions are `0`.  \n",
    "\n",
    "**Example:**  \n",
    "- Vocabulary: `[\"cat\", \"dog\", \"bat\"]`  \n",
    "- `\"cat\"` $\\to$ `[1, 0, 0]`  \n",
    "- `\"dog\"` $\\to$ `[0, 1, 0]`  \n",
    "- `\"bat\"` $\\to$ `[0, 0, 1]`  \n",
    "\n",
    "**Advantages:**  \n",
    "- Simple and easy to implement.  \n",
    "- Works well for very small vocabularies.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- High dimensionality: if the vocabulary has 10,000 words, the vector length will also be 10,000.  \n",
    "- Sparse representation (mostly zeros).  \n",
    "- No semantic meaning: words like `\"cat\"` and `\"dog\"` are represented as completely independent, with no relationship captured.  \n",
    "- Cannot handle new words (out-of-vocabulary issue).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209cce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " [array(['bat', 'cat', 'dog'], dtype='<U3')] \n",
      "\n",
      "One hot vectors :\n",
      " [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample words\n",
    "words = np.array([\"cat\", \"dog\", \"bat\", \"dog\", \"cat\"]).reshape(-1, 1)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform\n",
    "one_hot = encoder.fit_transform(words)\n",
    "\n",
    "print(\"Vocabulary: \\n\", encoder.categories_, \"\\n\")\n",
    "print(\"One hot vectors :\\n\", one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee2e32",
   "metadata": {},
   "source": [
    "#### Bag of Words (BoW)\n",
    "\n",
    "- **Definition:** Represents documents as vectors of word frequency counts.  \n",
    "- **Vocabulary:** Built from all unique words in the dataset.  \n",
    "- **Conversion:** Each document is transformed into a vector of counts corresponding to the vocabulary.  \n",
    "- **Limitation:** Ignores the order of words and syntactic/semantic relationships.  \n",
    "\n",
    "**Example:**  \n",
    "Documents:  \n",
    "1. `\"I love dogs\"`  \n",
    "2. `\"I love cats\"`  \n",
    "\n",
    "Vocabulary: `[\"I\", \"love\", \"dogs\", \"cats\"]`  \n",
    "\n",
    "- Doc1 = `[1, 1, 1, 0]`  \n",
    "- Doc2 = `[1, 1, 0, 1]`  \n",
    "\n",
    "**Advantages:**  \n",
    "- Simple and intuitive.  \n",
    "- Easy to implement.  \n",
    "- Works well for small datasets and basic models.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- High-dimensional and sparse for large vocabularies.  \n",
    "- No semantic meaning (e.g., `\"dog\"` and `\"puppy\"` are unrelated).  \n",
    "- Cannot handle synonyms or polysemy (same word with different meanings).  \n",
    "- Sensitive to stopwords and noise.  \n",
    "- Ignores word order, so `\"dog bites man\"` and `\"man bites dog\"` look the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4aa6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'are' 'cats' 'dogs' 'great' 'i' 'love'] \n",
      "\n",
      "Bag of words representation :\n",
      " [[0 0 0 1 0 1 1]\n",
      " [0 0 1 0 0 1 1]\n",
      " [1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "# Sample corpus (documents)\n",
    "corpus = [\n",
    "    \"I love dogs\",\n",
    "    \"I love cats\",\n",
    "    \"Cats and dogs are great\"\n",
    "]\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# Fit and transform\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out(), \"\\n\")\n",
    "\n",
    "# Document-term matrix\n",
    "print(\"Bag of words representation :\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a8a39",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- **Key Concepts:**  \n",
    "  - **TF (Term Frequency):** How often a word appears in a document.  \n",
    "  - **IDF (Inverse Document Frequency):** How rare a word is across all documents.  \n",
    "  - **TF-IDF:** Highlights words that are frequent in one document but not common across all documents.  \n",
    "\n",
    "- **Formula (IDF):**  \n",
    "  $\\mathrm{IDF(word)} = \\log \\frac{\\text{Total Documents}}{1 + \\text{Documents containing word}}$\n",
    "\n",
    "**Example:**  \n",
    "- Doc1 = `\"I love NLP\"`  \n",
    "- Doc2 = `\"I love Deep learning\"`  \n",
    "- Vocabulary = `[I, love, NLP, Deep, learning]`  \n",
    "\n",
    "**Term Frequency (TF):**  \n",
    "- Doc1 = `[1, 1, 1, 0, 0]`  \n",
    "- Doc2 = `[1, 1, 0, 1, 1]`  \n",
    "\n",
    "**Inverse Document Frequency (IDF):**  \n",
    "- `\"I\"`: $\\log(2/2) = 0$ (appears in both docs)  \n",
    "- `\"love\"`: $\\log(2/2) = 0$ (appears in both docs)  \n",
    "- `\"NLP\"`: $\\log(2/1) \\approx 0.693$ (only in Doc1)  \n",
    "- `\"Deep\"`: $\\log(2/1) \\approx 0.693$ (only in Doc2)  \n",
    "- `\"learning\"`: $\\log(2/1) \\approx 0.693$ (only in Doc2)  \n",
    "\n",
    "**TF-IDF = TF × IDF:**  \n",
    "- Doc1 = `[0, 0, 0.693, 0, 0]`  \n",
    "- Doc2 = `[0, 0, 0, 0.693, 0.693]`  \n",
    "\n",
    "Now the model gives higher weight to `\"NLP\"` in Doc1 and `\"Deep\"` & `\"learning\"` in Doc2, while common words like `\"I\"` and `\"love\"` are ignored.  \n",
    "\n",
    "**Advantages:**  \n",
    "- Reduces the importance of common words (stopwords).  \n",
    "- Useful for information retrieval, search engines, and keyword extraction.  \n",
    "- Simple and interpretable.  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- Still ignores word order and context.  \n",
    "- Rare words may get high weights even if not meaningful.  \n",
    "- Weights are dataset-dependent (new documents may change scores).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8008764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       deep         i  learning      love       nlp\n",
      "0  0.000000  0.501549  0.000000  0.501549  0.704909\n",
      "1  0.576152  0.409937  0.576152  0.409937  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"I love NLP\",\n",
    "    \"I love deep learning\",\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Convert document for clarity\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044f37c",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "- **Definition:** A way of representing words as dense, low-dimensional vectors where semantic meaning and relationships between words are captured.  \n",
    "- **Core Idea:**  \n",
    "  - Similar words → vectors close together.  \n",
    "  - Dissimilar words → vectors farther apart.  \n",
    "- **Need:** Captures semantic and syntactic relationships beyond simple frequency counts.  \n",
    "\n",
    "**Key Properties:**  \n",
    "- Each word is represented by a learned vector.  \n",
    "- Dimensions are not predefined but learned automatically during training.  \n",
    "- Words used in similar contexts tend to have similar vectors (distributional hypothesis).  \n",
    "\n",
    "**Examples of Word Embedding Models:**  \n",
    "\n",
    "1. **Word2Vec**  \n",
    "   - Two training methods:  \n",
    "     - **CBOW (Continuous Bag of Words):** Predicts a target word from its surrounding context.  \n",
    "     - **Skip-Gram:** Predicts surrounding context given a target word.  \n",
    "\n",
    "2. **GloVe (Global Vectors)**  \n",
    "   - Learns embeddings using word co-occurrence statistics across the entire corpus.  \n",
    "   - Captures both local (context-based) and global statistical information.  \n",
    "\n",
    "3. **FastText**  \n",
    "   - Extension of Word2Vec that uses subword (character n-grams) information.  \n",
    "   - Advantage: Can generate embeddings for out-of-vocabulary words (e.g., rare or misspelled words).  \n",
    "\n",
    "**Limitations:**  \n",
    "- **Static embeddings:** Each word has only one vector.  \n",
    "  - Example: `\"bank\"` (river bank vs. money bank) gets the same representation.  \n",
    "- Cannot fully capture polysemy (multiple meanings of the same word).  \n",
    "- Do not adapt based on sentence context.  \n",
    "\n",
    "**Advantages:**  \n",
    "- Dense, low-dimensional, and computationally efficient compared to one-hot vectors.  \n",
    "- Capture semantic relationships:  \n",
    "  - `\"king\" – \"man\" + \"woman\" ≈ \"queen\"`  \n",
    "- Widely used as pretrained embeddings (e.g., Google’s Word2Vec, Stanford’s GloVe).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062eceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'nlp':\n",
      " [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n",
      "\n",
      "Most similar to 'nlp':\n",
      "[('love', 0.16563552618026733), ('learning', 0.1267007291316986), ('powerful', 0.08872983604669571), ('is', 0.011071977205574512), ('i', -0.027841337025165558), ('deep', -0.15515567362308502), ('fun', -0.2187293916940689)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"nlp\"],\n",
    "    [\"i\", \"love\", \"deep\", \"learning\"],\n",
    "    [\"nlp\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"powerful\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"Vector for 'nlp':\\n\", model.wv['nlp'])\n",
    "\n",
    "# Find most similar words\n",
    "print(\"\\nMost similar to 'nlp':\")\n",
    "print(model.wv.most_similar('nlp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e39da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer': [ 0.079084 -0.81504   1.7901    0.91653   0.10797  -0.55628  -0.84427\n",
      " -1.4951    0.13418   0.63627 ]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained GloVe (example: 100d)\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_glove(path):\n",
    "    embeddings = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove = load_glove(\"../glove/glove.6B.50d.txt\")\n",
    "print(\"Vector for 'computer':\", glove[\"computer\"][:10])  # first 10 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c12867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      " [-1.0521135e-03 -2.9445016e-05 -2.6681324e-04 -2.0629806e-03\n",
      " -1.5533755e-03  1.5935432e-03  2.3113489e-03 -8.0846180e-04\n",
      " -3.6553531e-03  1.6169866e-03  2.6812166e-04 -1.2354901e-03\n",
      "  2.0685391e-05  1.8805226e-03  1.1954642e-03 -1.3146290e-03\n",
      "  1.6609058e-03 -8.6206105e-04 -3.7580152e-04  3.7063458e-03\n",
      "  1.4002168e-03 -1.0286489e-03 -3.4085761e-03  2.4692728e-03\n",
      "  1.8361167e-03 -2.4178838e-03 -2.5847720e-03  2.7235423e-03\n",
      " -1.2194831e-03  5.0885845e-03  3.2921617e-03  2.9445691e-03\n",
      " -2.5521258e-03  1.1273614e-03 -8.8686880e-04 -1.2647441e-03\n",
      "  9.6110343e-05  1.0053181e-03  4.1550426e-03  4.9981056e-03\n",
      "  3.1284827e-03  2.3335047e-04  3.5504177e-03  2.7294530e-04\n",
      " -1.5858869e-03 -1.0548470e-03 -3.9235768e-03 -4.2642830e-03\n",
      " -2.5969148e-03  9.5642585e-04]\n",
      "Most similar to 'learning':\n",
      " [('A', 0.11139702051877975), (' ', 0.10953480750322342), ('d', 0.06675366312265396), ('h', 0.057259414345026016), ('.', 0.027056237682700157), ('p', 0.015873564407229424), ('s', 0.012552175670862198), ('i', -0.01475050300359726), ('n', -0.022550400346517563), ('l', -0.029069049283862114)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(sentences, vector_size=50, window=3, min_count=1)\n",
    "\n",
    "print(\"Vector for 'learning':\\n\", model.wv['learning'])\n",
    "print(\"Most similar to 'learning':\\n\", model.wv.most_similar(\"learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1540c",
   "metadata": {},
   "source": [
    "### Contextual Embeddings\n",
    "\n",
    "- **Definition:** Word representations that change depending on the surrounding context (sentence/paragraph).  \n",
    "- **Key Idea:** Unlike static embeddings (Word2Vec, GloVe), the same word can have different vectors depending on its meaning in context.  \n",
    "\n",
    "**Examples of Models:**  \n",
    "- **ELMo (Embeddings from Language Models):** Generates context-dependent embeddings using bidirectional LSTMs.  \n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):** Uses transformer architecture to generate contextual embeddings in both directions.  \n",
    "- **GPT (Generative Pre-trained Transformer):** Produces contextual embeddings in a unidirectional (left-to-right) manner.  \n",
    "\n",
    "**Example Sentences:**  \n",
    "- `\"I deposited money in the bank\"` → vector close to **finance**.  \n",
    "- `\"We sat by the bank of the river\"` → vector close to **nature**.  \n",
    "\n",
    "**Uses / Advantages:**  \n",
    "- Captures **polysemy** (multiple meanings of the same word).  \n",
    "- Understands **word order, syntax, and grammar**.  \n",
    "- Provides **dynamic embeddings** that adapt to context.  \n",
    "- Learned using **large neural networks** (LSTMs, Transformers) trained on massive corpora.  \n",
    "- Powers modern NLP tasks:  \n",
    "  - Machine Translation  \n",
    "  - Question Answering  \n",
    "  - Sentiment Analysis  \n",
    "  - Named Entity Recognition  \n",
    "\n",
    "**Limitations:**  \n",
    "- Computationally expensive to train and use.  \n",
    "- Requires large datasets and high-performance hardware.  \n",
    "- Interpretability is lower compared to simple models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46213089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mht/.my-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-03 19:51:47.092927: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-03 19:51:47.518341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 19:51:50.197123: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: I ate an apple.\n",
      "Token: apple\n",
      "Embedding vector shape: torch.Size([768])\n",
      "First 5 values: tensor([ 0.1211,  0.7320, -0.5054, -0.6165,  1.0468])\n",
      "\n",
      "Sentence: Apple released a new iPhone.\n",
      "Token: apple\n",
      "Embedding vector shape: torch.Size([768])\n",
      "First 5 values: tensor([ 0.5733,  0.1726, -0.2070, -0.3598,  0.6186])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT Model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sentences with \"Apple\" in different meanings\n",
    "sentences = [\n",
    "    \"I ate an apple.\",\n",
    "    \"Apple released a new iPhone.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get embeddings from BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # outputs[0] = last hidden states for each token\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Extract embeddings for the word \"apple\"\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][i])\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if \"apple\" in token.lower():  # match token containing \"apple\"\n",
    "            print(f\"Token: {token}\")\n",
    "            print(f\"Embedding vector shape: {last_hidden_states[i, idx].shape}\")\n",
    "            print(f\"First 5 values: {last_hidden_states[i, idx][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610ff14",
   "metadata": {},
   "source": [
    "## Pre trained embeddings in Keras\n",
    "\n",
    "- Keras allows loading pre-trained embeddings into neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50eeb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'i': 1, 'love': 2, 'nlp': 3, 'is': 4, 'fun': 5, 'machine': 6, 'learning': 7}\n",
      "Sequences: [[1, 2, 3], [3, 4, 5], [1, 2, 6, 7]]\n",
      "Padded Sequences:\n",
      " [[1 2 3 0]\n",
      " [3 4 5 0]\n",
      " [1 2 6 7]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "docs = [\"I love NLP\", \"NLP is fun\", \"I love machine learning\"]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Word Index:\", word_index)\n",
    "print(\"Sequences:\", sequences)\n",
    "\n",
    "# Padding\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(\"Padded Sequences:\\n\", padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c168761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# Use Case: Sentiment Analysis (with TF-IDF + Logistic Regression)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "texts = [\"I love this movie\", \"This film is terrible\", \"Amazing storyline\", \"Worst acting ever\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Test\n",
    "test = [\"I love this film\", \"movie is worst\"]\n",
    "X_test = vectorizer.transform(test)\n",
    "print(model.predict(X_test))  # [1, 0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
