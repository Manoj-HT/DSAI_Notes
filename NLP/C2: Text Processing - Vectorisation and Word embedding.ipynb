{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a564b325",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING - VECTORISATION AND WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dba9d5",
   "metadata": {},
   "source": [
    "## Vectorisation\n",
    "\n",
    "- **Definition :** It is the process of converting text (words, sentences, documents) into numerical representations (vectors) so that the ML model can process them\n",
    "- Need: machines can't understand raw text, they work with numbers\n",
    "\n",
    "### Terminologies\n",
    "\n",
    "- Corpus: A collection of documents\n",
    "- Vocabulary: Unique set of words in corpus\n",
    "- Document-term matrix: Representation where rows are documents and columns are words\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- **Definition :** It is the process if breaking text into smaller units before giving it to a model\n",
    "- A token can be a word, subword or even a character depending in the toeknizer\n",
    "- The modelconverts each token intoanumber (ID) whichmaps to an embedding vector\n",
    "\n",
    "#### Need of tokenization\n",
    "\n",
    "- Consistent way to chop up text\n",
    "- Vocabulary list mapping tokens to numbers\n",
    "\n",
    "#### Types of tokenization\n",
    "\n",
    "1. Word level tokenization\n",
    "    - Splits text by spaces/punctuation\n",
    "    - Eg: \"I love NLP\" $\\to$ [\"I\", \"love\", \"NLP\"]\n",
    "    - Problem: Huge vocabulary, can't handle new/rare words\n",
    "\n",
    "2. Character level tokenization\n",
    "    - Each character is a token\n",
    "    - \"Chat\" $\\to$ [\"C\", \"h\", \"a\", \"t\"]\n",
    "    - Very flexible, but sequence becomes long\n",
    "\n",
    "3. Subword-level tokenization\n",
    "    - Breaks rare words into smaller units while keeping common words intact\n",
    "    - \"unhappiness\" $\\to$ [\"un\", \"##happi\", \"##ness\"]\n",
    "        - `##`: this piece continues a word\n",
    "    - Advantage: Can handle new words by combining pieces\n",
    "\n",
    "### Common techniques of Vectorisation\n",
    "\n",
    "Some common techniques to vectorize text\n",
    "- One hot encoding\n",
    "- Bag of words\n",
    "- TF-IDF\n",
    "- Word embeddings\n",
    "- Contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63823e89",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "\n",
    "- Each word in the vocabulary is represented as a vector of size equal to the vocabulary\n",
    "- only one position = 1, all others are considered 0\n",
    "- Eg: Consider a vocabulary <br>\n",
    "    - vocab = [\"cat\", \"dog\", \"bat\"]\n",
    "    - \"cat\" $\\to$ [1, 0, 0]\n",
    "    - \"dog\" $\\to$ [0, 1, 0]\n",
    "    - \"bat\" $\\to$ [0, 0, 1]\n",
    "- Advantages\n",
    "    - Simple and easy\n",
    "    - good for small vocabularies\n",
    "- Disadvantages\n",
    "    - If vocab = 10000 words, then vector length would be 10000, which is very sparse and memory heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209cce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " [array(['bat', 'cat', 'dog'], dtype='<U3')] \n",
      "\n",
      "One hot vectors :\n",
      " [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample words\n",
    "words = np.array([\"cat\", \"dog\", \"bat\", \"dog\", \"cat\"]).reshape(-1, 1)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform\n",
    "one_hot = encoder.fit_transform(words)\n",
    "\n",
    "print(\"Vocabulary: \\n\", encoder.categories_, \"\\n\")\n",
    "print(\"One hot vectors :\\n\", one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee2e32",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "\n",
    "- Represents documents as word frequency counts\n",
    "- Vocabulary is built from all unique words in the dataset\n",
    "- Each document is conberted into a vector of countd\n",
    "- Order of words is ignored\n",
    "- Eg: \n",
    "    - Document:\n",
    "        1. \"I love dogs\"\n",
    "        2. \"I love cats\"\n",
    "    - Vocabulary will be [\"I\", \"love\", \"dogs\", \"cats\"]\n",
    "    - Doc1 = [1, 1, 1, 0]\n",
    "    - Doc2 = [1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4aa6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'are' 'cats' 'dogs' 'great' 'i' 'love'] \n",
      "\n",
      "Bag of words representation :\n",
      " [[0 0 0 1 0 1 1]\n",
      " [0 0 1 0 0 1 1]\n",
      " [1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "# Sample corpus (documents)\n",
    "corpus = [\n",
    "    \"I love dogs\",\n",
    "    \"I love cats\",\n",
    "    \"Cats and dogs are great\"\n",
    "]\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# Fit and transform\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out(), \"\\n\")\n",
    "\n",
    "# Document-term matrix\n",
    "print(\"Bag of words representation :\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a8a39",
   "metadata": {},
   "source": [
    "### Term frequency - Inverse document frequency\n",
    "\n",
    "- Key concept:\n",
    "    - TF: How often a word appears in the document\n",
    "    - IDF: How rare a word is across all documents\n",
    "    - TF-IDF: Highlights words that are frequent in one document but are not common everywhere\n",
    "- Formula: Inverse document frequency\n",
    "    - $\\mathrm{IDF(word)} = log \\frac{Total docs}{1 + Docs\\;containing\\;word}$\n",
    "- Eg:\n",
    "    - Doc1 = \"I love NLP\"\n",
    "    - Doc2 = \"I love Deep learning\"\n",
    "    - vocab = (I, love, NLP, Deep, learning)\n",
    "    - Term frequency\n",
    "        - Doc1 = [1, 1, 1, 0, 0]\n",
    "        - Doc2 = [1, 1, 0, 1, 1]\n",
    "    - Inverse document frequency (Formula)\n",
    "        - \"I\": log(2/2) = 0 (appears in both docs)\n",
    "        - \"love\": log(2/2) = 0 (appears in both docs)\n",
    "        - \"NLP\": log(2/1) $\\approx$ 0.693 (only in Doc1)\n",
    "        - \"Deep\": log(2/1) $\\approx$ 0.693 (only in Doc2)\n",
    "        - \"learning\": log(2/1) $\\approx$ 0.693 (only in Doc2)\n",
    "    - TF-IDF: TF $\\times$ IDF\n",
    "        - Doc1 = [(1 $\\times$ 0), (1 $\\times$ 0), (1 $\\times$ 0.693), (1 $\\times$ 0), (1 $\\times$ 0)] <br>\n",
    "        => [0, 0, 0.693, 0]\n",
    "        - Doc2 = [(1 $\\times$ 0), (1 $\\times$ 0), (1 $\\times$ 0), (1 $\\times$ 0.693), (1 $\\times$ 0.693)] <br>\n",
    "        => [0, 0, 0, 0.693, 0.693]\n",
    "    - Now the model knows that \"NLP\" from Doc1 and \"deep\" & \"learning\" in Doc2 are special\n",
    "    - common words are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8008764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       deep         i  learning      love       nlp\n",
      "0  0.000000  0.501549  0.000000  0.501549  0.704909\n",
      "1  0.576152  0.409937  0.576152  0.409937  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"I love NLP\",\n",
    "    \"I love deep learning\",\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Convert document for clarity\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044f37c",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "- **Definition :** A way of representing words as dense vectors where semantic meaning and relationships between words are captured\n",
    "- similar words: vectors close together\n",
    "- dissimilar words: vectors far apart\n",
    "- Need: captures semantic relationships\n",
    "- Key Idea:\n",
    "    - Each word is represented by a vector\n",
    "    - The dimensions are learned automatically\n",
    "    - Context matters as words used in similar contexts have similar vectors\n",
    "- Eg:\n",
    "    - Word2Vec\n",
    "        1. CBOW: Continuous bag of words, predicts word from its context\n",
    "        2. Skip-Gram: predicts context from a word\n",
    "    - GloVe: Global vectors, Uses word's co occurence statistic accross the whole corpus\n",
    "    - FastText: similar to Word2Vec but uses subword information\n",
    "- Limitation: static embedding\n",
    "    - each word has only one vector (eg: bank for river bank and money bank is the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062eceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'nlp':\n",
      " [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n",
      "\n",
      "Most similar to 'nlp':\n",
      "[('love', 0.16563552618026733), ('learning', 0.1267007291316986), ('powerful', 0.08872983604669571), ('is', 0.011071977205574512), ('i', -0.027841337025165558), ('deep', -0.15515567362308502), ('fun', -0.2187293916940689)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"nlp\"],\n",
    "    [\"i\", \"love\", \"deep\", \"learning\"],\n",
    "    [\"nlp\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"powerful\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"Vector for 'nlp':\\n\", model.wv['nlp'])\n",
    "\n",
    "# Find most similar words\n",
    "print(\"\\nMost similar to 'nlp':\")\n",
    "print(model.wv.most_similar('nlp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1540c",
   "metadata": {},
   "source": [
    "### Contextual embeddings\n",
    "\n",
    "- Depending on the surroung words, this gives a different vector for the same word\n",
    "- Model Eg: BERT, ELMo, GPT\n",
    "- Eg: \n",
    "    - I deposited money in the bank <br>\n",
    "    -> Vector close to finance\n",
    "    - We sat by the bank of river <br>\n",
    "    -> Vector closer to nature\n",
    "- Uses:\n",
    "    - They capture polysemy\n",
    "    - They understand word order and grammar\n",
    "    - They are learned by Large nural networks trained on massive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46213089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: I ate an apple.\n",
      "Token: apple\n",
      "Embedding vector shape: torch.Size([768])\n",
      "First 5 values: tensor([ 0.1211,  0.7320, -0.5054, -0.6165,  1.0468])\n",
      "\n",
      "Sentence: Apple released a new iPhone.\n",
      "Token: apple\n",
      "Embedding vector shape: torch.Size([768])\n",
      "First 5 values: tensor([ 0.5733,  0.1726, -0.2070, -0.3598,  0.6186])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT Model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sentences with \"Apple\" in different meanings\n",
    "sentences = [\n",
    "    \"I ate an apple.\",\n",
    "    \"Apple released a new iPhone.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get embeddings from BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # outputs[0] = last hidden states for each token\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Extract embeddings for the word \"apple\"\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][i])\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if \"apple\" in token.lower():  # match token containing \"apple\"\n",
    "            print(f\"Token: {token}\")\n",
    "            print(f\"Embedding vector shape: {last_hidden_states[i, idx].shape}\")\n",
    "            print(f\"First 5 values: {last_hidden_states[i, idx][:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
