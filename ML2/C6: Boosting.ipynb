{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9676dbef",
   "metadata": {},
   "source": [
    "# C6: BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29538c3",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- **Definition:** Boosting is an ensemble learning technique that combines multiple weak learners to form a strong learner.  \n",
    "- **Goal:** Improve accuracy by focusing more on the errors made by previous models.  \n",
    "- **Key idea:** Each new model is trained to correct the mistakes of the prior ones.  \n",
    "- **Applications:** Classification, regression, ranking, anomaly detection.  \n",
    "\n",
    "### Common Algorithms\n",
    "\n",
    "- AdaBoost (Adaptive Boosting)  \n",
    "- Gradient Boosting  \n",
    "- XGBoost  \n",
    "- LightGBM  \n",
    "- CatBoost  \n",
    "\n",
    "### Working Steps\n",
    "\n",
    "1. Train a weak learner.  \n",
    "2. Evaluate its errors.  \n",
    "3. Increase the weight/importance of misclassified points.  \n",
    "4. Train the next learner focusing on those hard examples.  \n",
    "5. Repeat for multiple learners.  \n",
    "6. Combine all learners into a weighted majority vote or weighted sum.  \n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Converts weak models into strong ones.  \n",
    "- Often achieves state-of-the-art accuracy.  \n",
    "- Handles both linear and complex non-linear data.  \n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Prone to overfitting if not tuned well.  \n",
    "- Computationally more expensive compared to bagging methods like Random Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.8366666666666667\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost example\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                           n_redundant=5, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base learner (weak learner)\n",
    "base_learner = DecisionTreeClassifier(max_depth=1)  # Decision stump\n",
    "\n",
    "# Define AdaBoost model\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=base_learner,\n",
    "    n_estimators=50,       # number of weak learners\n",
    "    learning_rate=1.0,     # step size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ada_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ae99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "# Gradient boost example\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                           n_redundant=5, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,     # number of trees\n",
    "    learning_rate=0.1,    # shrinkage / step size\n",
    "    max_depth=3,          # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44491ec",
   "metadata": {},
   "source": [
    "|                    | AdaBoost                                                                                                 | Gradient Boost                                                                                                                          |\n",
    "| ------------------ | -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Idea**           | Focuses on misclassified samples                                                                         | Focuses on residual errors                                                                                                              |\n",
    "| **How**            | Assigns higher weights to misclassified data points so the next weak learner pays more attention to them | Each new learner is trained to predict the residuals                                                                                    |\n",
    "| **Weak learners**  | Often uses decision stumps                                                                               | Typically deeper decision trees                                                                                                         |\n",
    "| **Loss functions** | Exponential loss                                                                                         | Very flexible, supports many differentiable loss functions                                                                              |\n",
    "| **Strengths**      | - Very simple and easy to implement <br> - Works well for binary classification                          | - More flexible and powerful <br> - Works for classification, regression, ranking <br> - Usually achieves higher accuracy than AdaBoost |\n",
    "| **Weaknesses**     | - Sensitive to outliers and noisy data <br> - Less flexible than GBM                                     | - More computationally expensive <br> - Needs careful tuning                                                                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b623c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3351.001637862091\n"
     ]
    }
   ],
   "source": [
    "# XGBoost: Xtreme Gradient Boosting\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost model\n",
    "model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459edbe6",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "- **Definition:** Combining multiple different models and then using a meta-model to make the final prediction.  \n",
    "\n",
    "### Working\n",
    "\n",
    "- **Base models:** Each predicts on the dataset.  \n",
    "- **Meta model:**  \n",
    "    - Learns how to best combine the outputs of the base models.  \n",
    "    - Usually a simple model.  \n",
    "- **Process:**  \n",
    "    1. Train base models on the training data.  \n",
    "    2. Collect their predictions.  \n",
    "    3. Train the meta model on those predictions.  \n",
    "    4. For new data, base models make predictions, and the meta model provides the final prediction.  \n",
    "\n",
    "### Usage\n",
    "\n",
    "- Captures different strengths of models.  \n",
    "- Often performs better than individual models.  \n",
    "- Reduces the risk of relying on a single model's biases.  \n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Can overfit if not done carefully.  \n",
    "- Requires out-of-fold predictions for training the meta model.  \n",
    "- More computationally expensive.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f905d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define base learners\n",
    "base_learners = [\n",
    "    ('decision_tree', DecisionTreeClassifier(max_depth=3, random_state=42)),\n",
    "    ('svm', SVC(probability=True, kernel='linear', random_state=42))\n",
    "]\n",
    "\n",
    "# 3. Define meta-learner (final estimator)\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "# 4. Build Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner\n",
    ")\n",
    "\n",
    "# 5. Train\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Predict & Evaluate\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "print(\"Stacking Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d36406",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "- **Definition:** In this technique, multiple models are combined, and their predictions are aggregated by voting.  \n",
    "\n",
    "### Types of Voting\n",
    "\n",
    "1. **Hard Voting**  \n",
    "    - Each model votes for a class.  \n",
    "    - The final prediction is the class with the majority of votes.  \n",
    "\n",
    "2. **Soft Voting**  \n",
    "    - Models provide class probabilities instead of just votes.  \n",
    "    - The probabilities are averaged, and the class with the highest average probability is chosen.  \n",
    "    - Usually performs better if the models can output probabilities.  \n",
    "\n",
    "3. **Voting Regressor**  \n",
    "    - Instead of predicting a class, predictions are averaged across models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9ada022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define base learners\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 3. Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
    "    voting='hard'   # change to 'soft' for soft voting\n",
    ")\n",
    "\n",
    "# 4. Train\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict & Evaluate\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
