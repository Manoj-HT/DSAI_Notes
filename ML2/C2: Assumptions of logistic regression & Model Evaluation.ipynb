{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47b0fd8",
   "metadata": {},
   "source": [
    "# C2: ASSUMPTIONS OF LOGISTIC REGRESSION & MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d0f34",
   "metadata": {},
   "source": [
    "## Assumptions of Logistic Regression\n",
    "Before performing Logistic Regression, we must check if certain conditions hold:\n",
    "\n",
    "1. **Binary outcome**\n",
    "    - The dependent variable must be binary (0/1, yes/no, true/false).\n",
    "    - For multiclass problems, categorical extensions like *multinomial logistic regression* should be used.\n",
    "\n",
    "2. **Independence of observations**\n",
    "    - Each data point should be independent.\n",
    "    - Example: A patient’s record should not influence another patient’s record.\n",
    "\n",
    "3. **Linearity of Log-Odds**\n",
    "    - Logistic regression assumes that the log(odds) has a linear relationship with the predictors.\n",
    "    - Example: $\\mathrm{log(odds)} = \\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{blood pressure}$\n",
    "\n",
    "4. **No multicollinearity among predictors**\n",
    "    - Predictors should not be highly correlated with each other.\n",
    "    - Example: Height in cm and height in inches (redundant values).\n",
    "\n",
    "5. **Large sample size**\n",
    "    - Logistic regression works best with large datasets.\n",
    "    - This is especially important when the event of interest is rare.\n",
    "\n",
    "## Model Evaluation Metrics\n",
    "- Logistic regression is primarily used for classification.\n",
    "- We need to evaluate performance using multiple metrics, not just accuracy.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "### Metrics\n",
    "\n",
    "1. **Accuracy**\n",
    "    - $\\mathrm{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "    - Shows overall correctness of the model.\n",
    "    - Can be misleading in imbalanced datasets (e.g., when one class dominates).\n",
    "\n",
    "2. **Precision**\n",
    "    - $\\mathrm{Precision} = \\frac{TP}{TP + FP}$\n",
    "    - Focus: Correctness of positive predictions.\n",
    "    - Tells how many predicted positives were actually correct.\n",
    "    - Example: Out of 100 predicted \"cancer\" cases, 80 were correct → precision = 0.8.\n",
    "\n",
    "3. **Recall (Sensitivity / True Positive Rate)**\n",
    "    - $\\mathrm{Recall} = \\frac{TP}{TP + FN}$\n",
    "    - Focus: Capturing actual positives.\n",
    "    - Tells how many of the real positive cases were correctly identified.\n",
    "    - Example: Out of 100 actual cancer cases, the model detected 90 → recall = 0.9.\n",
    "\n",
    "4. **F1 Score**\n",
    "    - $\\mathrm{F1} = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$\n",
    "    - Harmonic mean of precision and recall.\n",
    "    - Useful when there is class imbalance and we want a balance between precision and recall.\n",
    "\n",
    "5. **ROC Curve & Area Under Curve (AUC)**\n",
    "    - ROC curve plots True Positive Rate (Recall) vs. False Positive Rate.\n",
    "    - AUC represents the area under this curve.\n",
    "    - Interpretation:\n",
    "        - AUC = 0.5 → Model is no better than random guessing.\n",
    "        - AUC close to 1 → Model is very good at distinguishing between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810bf4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : [[2 0]\n",
      " [0 1]]\n",
      "Accuracy : 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "F1 score : 1.0\n",
      "ROC-AUC : 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, 32, 47, 51, 62, 23, 34, 45, 36, 50],\n",
    "    'bp': [120, 130, 140, 150, 160, 110, 125, 135, 128, 142],\n",
    "    'has_disease': [0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "X = data[['age', 'bp']]\n",
    "y = data['has_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Confusion Matrix :\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision :\", precision_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Recall :\", recall_score(y_test, y_pred, zero_division=0))\n",
    "print(\"F1 score :\", f1_score(y_test, y_pred, zero_division=0))\n",
    "print(\"ROC-AUC :\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fcef5",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "Consider an example with a dataset of 100 patients:\n",
    "- 95 healthy (class 0)\n",
    "- 5 with disease (class 1)\n",
    "\n",
    "If a model always predicts \"healthy,\" it will achieve 95% accuracy.  \n",
    "However, this is misleading because it fails to identify any diseased patients.  \n",
    "\n",
    "This is why we use additional metrics such as **Precision, Recall, F1-score, and ROC-AUC**, instead of relying only on accuracy.\n",
    "\n",
    "## Handling Imbalanced Data\n",
    "\n",
    "When one class is much rarer than the other, models tend to ignore the minority class.  \n",
    "Examples include **fraud detection, cancer diagnosis, churn prediction, and rare event forecasting**.\n",
    "\n",
    "### Techniques to Handle Imbalance\n",
    "\n",
    "1. **Resampling**\n",
    "    - **Oversampling the minority class**: Duplicate rare samples or create synthetic ones.\n",
    "    - **Undersampling the majority class**: Randomly drop some frequent class samples.\n",
    "    - **Trade-off**: Oversampling may cause overfitting; undersampling can result in loss of useful information.\n",
    "\n",
    "2. **Synthetic Minority Oversampling Technique (SMOTE)**\n",
    "    - Creates new synthetic samples for the minority class by interpolating between existing samples.\n",
    "    - Balances the dataset more effectively than simple duplication.\n",
    "    - **Caution**: May create overlapping or noisy samples if not tuned carefully.\n",
    "\n",
    "3. **Class Weight Adjustment**\n",
    "    - Assign higher importance (weight) to the minority class during training.\n",
    "    - In Logistic Regression, this can be done using `class_weight='balanced'`.\n",
    "    - Helps the model pay more attention to rare events.\n",
    "\n",
    "4. **Threshold Tuning**\n",
    "    - Logistic regression outputs probabilities by default.\n",
    "    - The usual cutoff is **0.5**, but in imbalanced problems, lowering the threshold can help capture more positives.\n",
    "    - **Trade-off**: Lowering the threshold increases Recall but reduces Precision.\n",
    "\n",
    "5. **Evaluation with Proper Metrics**\n",
    "    - In imbalanced datasets, Accuracy is not reliable.\n",
    "    - Metrics like **Precision-Recall Curve, F1-score, ROC-AUC, and Matthews Correlation Coefficient (MCC)** are more informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729bb07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without class weight :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       269\n",
      "           1       0.70      0.45      0.55        31\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.82      0.71      0.75       300\n",
      "weighted avg       0.91      0.92      0.92       300\n",
      "\n",
      "With class weight :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       269\n",
      "           1       0.45      0.84      0.58        31\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.71      0.86      0.76       300\n",
      "weighted avg       0.92      0.88      0.89       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with class weights\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "model_normal = LogisticRegression(max_iter=1000)\n",
    "model_normal.fit(X_train, y_train)\n",
    "print(\"Without class weight :\", classification_report(y_test, model_normal.predict(X_test)))\n",
    "\n",
    "model_balanced = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model_balanced.fit(X_train, y_train)\n",
    "print(\"With class weight :\", classification_report(y_test, model_balanced.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2b2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE : 0    626\n",
      "1     74\n",
      "Name: count, dtype: int64\n",
      "After SMOTE : 0    626\n",
      "1    626\n",
      "Name: count, dtype: int64\n",
      "Confustion matrix : [[237  32]\n",
      " [  5  26]]\n",
      "Classification report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       269\n",
      "           1       0.45      0.84      0.58        31\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.71      0.86      0.76       300\n",
      "weighted avg       0.92      0.88      0.89       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SMOTE oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE :\", pd.Series(y_train).value_counts())\n",
    "print(\"After SMOTE :\", pd.Series(y_train_res).value_counts())\n",
    "\n",
    "model_smote = LogisticRegression(max_iter=1000)\n",
    "model_smote.fit(X_train_res,y_train_res)\n",
    "y_pred = model_smote.predict(X_test)\n",
    "\n",
    "print(\"Confustion matrix :\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report :\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
