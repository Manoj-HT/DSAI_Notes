{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26209353",
   "metadata": {},
   "source": [
    "# C4: DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bced19",
   "metadata": {},
   "source": [
    "## Decsion tree\n",
    "\n",
    "## Terminologies:\n",
    "\n",
    "### Root node\n",
    "\n",
    "- It is the top most node and the starting point of the tree\n",
    "- Represents the entire dataset before any splitting\n",
    "- Chosen based on the best feature that provides the highest information gain\n",
    "- All other branches grow from this node\n",
    "\n",
    "### Splitting\n",
    "\n",
    "- **Definition :** Dividing a node into two or more sub nodes based on chosen features/condition\n",
    "- **Purpose :** To make child nodes more pure\n",
    "- **Splitting criterion :** Depends on algorithm\n",
    "- **Process :**\n",
    "    - At a node, evaluate all features\n",
    "    - Select the feature/threshold that best seperates the data\n",
    "    - Create branches based on that decision\n",
    "\n",
    "### Leaf/Terminal node\n",
    "\n",
    "- A node which has no further splits\n",
    "- Represents the final outcome/decision of the path\n",
    "- All data points reaching this node are classified/regressed into a single value or category\n",
    "\n",
    "### Branch\n",
    "\n",
    "- A connection/link between a parent node and a child node\n",
    "- Represents the outcome of a condition/test applied at the parent node\n",
    "- Each branch corresponds to a decision path\n",
    "\n",
    "### Depth of a decision tree\n",
    "\n",
    "- The length of the longest path from the root node to leaf node\n",
    "- Root node has depth = 0\n",
    "- It shows how many decsions are made before reaching final prediction\n",
    "- A deeper tree means a more complex model\n",
    "\n",
    "## Entropy\n",
    "\n",
    "- A measure of impurity or disorder in a dataset\n",
    "- Higher entropy means data is very mixed \n",
    "- Lower entropy means data is more pure\n",
    "- Used in ID3/C4.5 algorithms to decide the best split\n",
    "\n",
    "### Formula:\n",
    "\n",
    "- $\\mathrm{Entropy(S)} = - \\sum_{i=1}^k p_i log_2 (p_i)$\n",
    "- k = number of classes\n",
    "- $p_i$ = proportion of class i in set S\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- If all samples belong to one class then Entropy = 0\n",
    "- If samples are split 50/50 between two classes then Entropy = 1\n",
    "\n",
    "### Shannons's entropy\n",
    "\n",
    "- Introduced by calude shannon\n",
    "- Measures the average amount of information in a random variable\n",
    "- Basis for entropy used in decision trees\n",
    "\n",
    "### Conditional entropy\n",
    "\n",
    "- It measures the amount of uncertainity remaining in a random variable Y given that we already know the value of another variable X\n",
    "- In other words, how much extra information is required to describe Y once X is known \n",
    "\n",
    "## Information gain\n",
    "\n",
    "- A measure of how much entropy is reduces after splitting a dataset based on a feature\n",
    "- Used in decsion trees to pick the best attribute for splitting\n",
    "\n",
    "### Formula\n",
    "\n",
    "- $\\mathrm{IG(Y, X)} = H(Y) - H(Y|X)$\n",
    "- H(Y): Entropy of the target before the split\n",
    "- H(Y|X): conditional entropy of the target after the split \n",
    "\n",
    "### Intuition\n",
    "\n",
    "- High IG: features gives a big reduction in uncertainity, this says that there has been a good split\n",
    "- Low IG: feature doesn't help much, this says that there has been a poor split"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
